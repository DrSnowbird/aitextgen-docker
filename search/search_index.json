{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"aitextgen \u00b6 Last Updated: April 18th, 2021 (aitextgen v0.5.0) A robust Python tool for text-based AI training and generation using OpenAI's GPT-2 and EleutherAI's GPT Neo/GPT-3 architecture. aitextgen is a Python package that leverages PyTorch , Hugging Face Transformers and pytorch-lightning with specific optimizations for text generation using GPT-2, plus many added features. It is the successor to textgenrnn and gpt-2-simple , taking the best of both packages: Finetunes on a pretrained 124M/355M/774M GPT-2 model from OpenAI or a 125M/355M GPT Neo model from EleutherAI...or create your own GPT-2/GPT Neo model + tokenizer and train from scratch! Generates text faster than gpt-2-simple and with better memory efficiency! With Transformers, aitextgen preserves compatibility with the base package, allowing you to use the model for other NLP tasks, download custom GPT-2 models from the HuggingFace model repository, and upload your own models! Also, it uses the included generate() function to allow a massive amount of control over the generated text. With pytorch-lightning, aitextgen trains models not just on CPUs and GPUs, but also multiple GPUs and (eventually) TPUs! It also includes a pretty training progress bar, with the ability to add optional loggers. The input dataset is its own object, allowing you to not only easily encode megabytes of data in seconds, cache, and compress it on a local computer before transporting to a remote server, but you are able to merge datasets without biasing the resulting dataset, or cross-train on multiple datasets to create blended output. Installation \u00b6 aitextgen can be installed from PyPI: pip3 install aitextgen More \u00b6 For more info on how to use aitextgen, check out the nav sidebar, or you can do a Hello World tutorial .","title":"Home"},{"location":"#aitextgen","text":"Last Updated: April 18th, 2021 (aitextgen v0.5.0) A robust Python tool for text-based AI training and generation using OpenAI's GPT-2 and EleutherAI's GPT Neo/GPT-3 architecture. aitextgen is a Python package that leverages PyTorch , Hugging Face Transformers and pytorch-lightning with specific optimizations for text generation using GPT-2, plus many added features. It is the successor to textgenrnn and gpt-2-simple , taking the best of both packages: Finetunes on a pretrained 124M/355M/774M GPT-2 model from OpenAI or a 125M/355M GPT Neo model from EleutherAI...or create your own GPT-2/GPT Neo model + tokenizer and train from scratch! Generates text faster than gpt-2-simple and with better memory efficiency! With Transformers, aitextgen preserves compatibility with the base package, allowing you to use the model for other NLP tasks, download custom GPT-2 models from the HuggingFace model repository, and upload your own models! Also, it uses the included generate() function to allow a massive amount of control over the generated text. With pytorch-lightning, aitextgen trains models not just on CPUs and GPUs, but also multiple GPUs and (eventually) TPUs! It also includes a pretty training progress bar, with the ability to add optional loggers. The input dataset is its own object, allowing you to not only easily encode megabytes of data in seconds, cache, and compress it on a local computer before transporting to a remote server, but you are able to merge datasets without biasing the resulting dataset, or cross-train on multiple datasets to create blended output.","title":"aitextgen"},{"location":"#installation","text":"aitextgen can be installed from PyPI: pip3 install aitextgen","title":"Installation"},{"location":"#more","text":"For more info on how to use aitextgen, check out the nav sidebar, or you can do a Hello World tutorial .","title":"More"},{"location":"cli/","text":"Command-Line Interface \u00b6 aitextgen has a command-line interface to quickly automate common tasks, and make it less necessary to use a script; helpful if running on a remote server. Encode \u00b6 Encodes given text text.txt into a cache and compressed TokenDataset , good for prepping a dataset for transit to a remote server. aitextgen encode text.txt If you are encoding a CSV, you should pass in the line_by_line parameter as well. aitextgen encode reddit.csv --line_by_line True Train \u00b6 To train/finetune on the default 124M GPT-2, given text text.txt and all default parameters: aitextgen train text.txt If you are using a cached/compressed dataset that ends with tar.gz (e.g one created by the Encoding CLI command above), you can pass that to this function as well. aitextgen train dataset_cache.tar.gz Other parameters to the TokenDataset constructor can be used. Generate \u00b6 Loads a model and generates to a file. By default, it will generate 20 texts to the file, 1 at a time at temperature of 0.7. aitextgen generate You can print to console instead by passing --to_file False aitextgen generate --prompt \"I believe in unicorns because\" --to_file False","title":"Command-Line Interface"},{"location":"cli/#command-line-interface","text":"aitextgen has a command-line interface to quickly automate common tasks, and make it less necessary to use a script; helpful if running on a remote server.","title":"Command-Line Interface"},{"location":"cli/#encode","text":"Encodes given text text.txt into a cache and compressed TokenDataset , good for prepping a dataset for transit to a remote server. aitextgen encode text.txt If you are encoding a CSV, you should pass in the line_by_line parameter as well. aitextgen encode reddit.csv --line_by_line True","title":"Encode"},{"location":"cli/#train","text":"To train/finetune on the default 124M GPT-2, given text text.txt and all default parameters: aitextgen train text.txt If you are using a cached/compressed dataset that ends with tar.gz (e.g one created by the Encoding CLI command above), you can pass that to this function as well. aitextgen train dataset_cache.tar.gz Other parameters to the TokenDataset constructor can be used.","title":"Train"},{"location":"cli/#generate","text":"Loads a model and generates to a file. By default, it will generate 20 texts to the file, 1 at a time at temperature of 0.7. aitextgen generate You can print to console instead by passing --to_file False aitextgen generate --prompt \"I believe in unicorns because\" --to_file False","title":"Generate"},{"location":"dataset/","text":"TokenDataset \u00b6 aitextgen has a special class, TokenDataset , used for managing tokenized datasets to be fed into model training. (this is in contrast with other GPT-2 finetuning approaches, which tokenizes at training time although you can still do that by passing a file_path and other relevant parameters to ai.train() .) This has a few nice bonuses, including: Tokenize a dataset on a local machine ahead of time and compress it, saving time/bandwidth transporting data to a remote machine Supports both reading a dataset line-by-line (including single-column CSVs), or bulk texts. Debug and log the loaded texts. Merge datasets together without using external libraries Cross-train on multiple datasets to \"blend\" them together. Creating a TokenDataset For GPT-2 Finetuning \u00b6 The easiest way to create a TokenDataset is to provide a target file. If no tokenizer_file is provided, it will use the default GPT-2 tokenizer. from aitextgen.TokenDataset import TokenDataset data = TokenDataset ( \"shakespeare.txt\" ) If you pass a single-column CSV and specify line_by_line=True , the TokenDataset will parse it row-by-row, and is the recommended way to handle multiline texts. data = TokenDataset ( \"politics.csv\" , line_by_line = True ) You can also manually pass a list of texts to texts instead if you've processed them elsewhere. data = TokenDataset ( texts = [ \"Lorem\" , \"Ipsum\" , \"Dolor\" ]) Block Size \u00b6 block_size is another parameter that can be passed when creating a TokenDataset, more useful for custom models. This should match the context window (e.g. the n_positions or max_position_embeddings config parameters). By default, it will choose 1024 : the GPT-2 context window. When implicitly loading a dataset via ai.train() , the block_size will be set to what is supported by the corresponding model config . Debugging a TokenDataset \u00b6 When loading a dataset, a progress bar will appear showing how many texts are loaded and If you want to see what exactly is input to the model during training, you can access a slice via data[0] . Saving/Loading a TokenDataset \u00b6 When creating a TokenDataset, you can automatically save it as a compressed gzipped numpy array when completed. data = TokenDataset ( \"shakespeare.txt\" , save_cache = True ) Or save it after you've loaded it with the save() function. data = TokenDataset ( \"shakespeare.txt\" ) data . save () By default, it will save to dataset_cache.tar.gz . You can then reload that into another Python session by specifying the cache. data = TokenDataset ( \"dataset_cache.tar.gz\" , from_cache = True ) CLI You can quickly create a Tokenized dataset using the command line, e.g. aitextgen encode text.txt . This will drastically reduce the file size, and is recommended before moving the file to cloud services (where it can be loaded using the from_cache parameter noted above) Using TokenDatasets with a Custom GPT-2 Model \u00b6 The default TokenDataset has a block_size of 1024 , which corresponds to the context window of the default GPT-2 model . If you're using a custom model w/ a different maximum. Additionally, you must explicitly provide the tokenizer file to rebuild the tokenizer, as the tokenizer will be different than the normal GPT-2 one. See the Model From Scratch docs for more info. Merging TokenDatasets \u00b6 Merging processed TokenDatasets can be done with the merge_datasets() function. By default, it will take samples equal to the smallest dataset from each TokenDataset, randomly sampling the appropriate number of texts from the larger datasets. This will ensure that model training does not bias toward one particular dataset. (it can be disabled by setting equalize=False ) (You can merge bulk datasets and line-by-line datasets, but the training output may be bizarre!) About Merging The current implementation merges by subset count, so equalization may not be perfect, but it will not significantly impact training. from aitextgen.TokenDataset import TokenDataset , merge_datasets data1 = TokenDataset ( \"politics1000.csv\" , line_by_line = True ) # 1000 samples data2 = TokenDataset ( \"politics4000.csv\" , line_by_line = True ) # 4000 samples data_merged = merge_datasets ([ data1 , data2 ]) # ~2000 samples","title":"TokenDataset"},{"location":"dataset/#tokendataset","text":"aitextgen has a special class, TokenDataset , used for managing tokenized datasets to be fed into model training. (this is in contrast with other GPT-2 finetuning approaches, which tokenizes at training time although you can still do that by passing a file_path and other relevant parameters to ai.train() .) This has a few nice bonuses, including: Tokenize a dataset on a local machine ahead of time and compress it, saving time/bandwidth transporting data to a remote machine Supports both reading a dataset line-by-line (including single-column CSVs), or bulk texts. Debug and log the loaded texts. Merge datasets together without using external libraries Cross-train on multiple datasets to \"blend\" them together.","title":"TokenDataset"},{"location":"dataset/#creating-a-tokendataset-for-gpt-2-finetuning","text":"The easiest way to create a TokenDataset is to provide a target file. If no tokenizer_file is provided, it will use the default GPT-2 tokenizer. from aitextgen.TokenDataset import TokenDataset data = TokenDataset ( \"shakespeare.txt\" ) If you pass a single-column CSV and specify line_by_line=True , the TokenDataset will parse it row-by-row, and is the recommended way to handle multiline texts. data = TokenDataset ( \"politics.csv\" , line_by_line = True ) You can also manually pass a list of texts to texts instead if you've processed them elsewhere. data = TokenDataset ( texts = [ \"Lorem\" , \"Ipsum\" , \"Dolor\" ])","title":"Creating a TokenDataset For GPT-2 Finetuning"},{"location":"dataset/#block-size","text":"block_size is another parameter that can be passed when creating a TokenDataset, more useful for custom models. This should match the context window (e.g. the n_positions or max_position_embeddings config parameters). By default, it will choose 1024 : the GPT-2 context window. When implicitly loading a dataset via ai.train() , the block_size will be set to what is supported by the corresponding model config .","title":"Block Size"},{"location":"dataset/#debugging-a-tokendataset","text":"When loading a dataset, a progress bar will appear showing how many texts are loaded and If you want to see what exactly is input to the model during training, you can access a slice via data[0] .","title":"Debugging a TokenDataset"},{"location":"dataset/#savingloading-a-tokendataset","text":"When creating a TokenDataset, you can automatically save it as a compressed gzipped numpy array when completed. data = TokenDataset ( \"shakespeare.txt\" , save_cache = True ) Or save it after you've loaded it with the save() function. data = TokenDataset ( \"shakespeare.txt\" ) data . save () By default, it will save to dataset_cache.tar.gz . You can then reload that into another Python session by specifying the cache. data = TokenDataset ( \"dataset_cache.tar.gz\" , from_cache = True ) CLI You can quickly create a Tokenized dataset using the command line, e.g. aitextgen encode text.txt . This will drastically reduce the file size, and is recommended before moving the file to cloud services (where it can be loaded using the from_cache parameter noted above)","title":"Saving/Loading a TokenDataset"},{"location":"dataset/#using-tokendatasets-with-a-custom-gpt-2-model","text":"The default TokenDataset has a block_size of 1024 , which corresponds to the context window of the default GPT-2 model . If you're using a custom model w/ a different maximum. Additionally, you must explicitly provide the tokenizer file to rebuild the tokenizer, as the tokenizer will be different than the normal GPT-2 one. See the Model From Scratch docs for more info.","title":"Using TokenDatasets with a Custom GPT-2 Model"},{"location":"dataset/#merging-tokendatasets","text":"Merging processed TokenDatasets can be done with the merge_datasets() function. By default, it will take samples equal to the smallest dataset from each TokenDataset, randomly sampling the appropriate number of texts from the larger datasets. This will ensure that model training does not bias toward one particular dataset. (it can be disabled by setting equalize=False ) (You can merge bulk datasets and line-by-line datasets, but the training output may be bizarre!) About Merging The current implementation merges by subset count, so equalization may not be perfect, but it will not significantly impact training. from aitextgen.TokenDataset import TokenDataset , merge_datasets data1 = TokenDataset ( \"politics1000.csv\" , line_by_line = True ) # 1000 samples data2 = TokenDataset ( \"politics4000.csv\" , line_by_line = True ) # 4000 samples data_merged = merge_datasets ([ data1 , data2 ]) # ~2000 samples","title":"Merging TokenDatasets"},{"location":"ethics/","text":"Ethics \u00b6 aitextgen is a tool primarily intended to help facilitate creative content. It is not a tool intended to deceive. Although parody accounts are an obvious use case for this package, make sure you are as upfront as possible with the methodology of the text you create. This includes: State that the text was generated using aitextgen and/or a GPT-2 model architecture. (a link to this repo would be a bonus!) If parodying a person, explicitly state that it is a parody, and reference who it is parodying. If the generated human-curated, or if it's unsupervised random output Indicating who is maintaining/curating the AI-generated text. Make a good-faith effort to remove overfit output from the generated text that matches the input text verbatim. It's fun to anthropomorphise the nameless \"AI\" as an abstract genius, but part of the reason I made aitextgen (and all my previous text-generation projects) is to make the technology more accessible and accurately demonstrate both its promise, and its limitations. Any AI text generation projects that are deliberately deceptive may be disavowed.","title":"Ethics"},{"location":"ethics/#ethics","text":"aitextgen is a tool primarily intended to help facilitate creative content. It is not a tool intended to deceive. Although parody accounts are an obvious use case for this package, make sure you are as upfront as possible with the methodology of the text you create. This includes: State that the text was generated using aitextgen and/or a GPT-2 model architecture. (a link to this repo would be a bonus!) If parodying a person, explicitly state that it is a parody, and reference who it is parodying. If the generated human-curated, or if it's unsupervised random output Indicating who is maintaining/curating the AI-generated text. Make a good-faith effort to remove overfit output from the generated text that matches the input text verbatim. It's fun to anthropomorphise the nameless \"AI\" as an abstract genius, but part of the reason I made aitextgen (and all my previous text-generation projects) is to make the technology more accessible and accurately demonstrate both its promise, and its limitations. Any AI text generation projects that are deliberately deceptive may be disavowed.","title":"Ethics"},{"location":"generate-performance/","text":"Improving Generation Performance \u00b6 A few tips and tricks for improving generation performance for both on CPUs and GPUs. (note that with these tricks, you cannot train the model afterwards!) CPU \u00b6 Quantization \u00b6 PyTorch has the ability to quantize models on the CPU. Currently, it will only quantize the Linear layer of GPT-2, but the generation performances increases 15% \u2014 25% ; far from trivial! To quantize a model after it's loaded, just run: ai . quantize () GPU \u00b6 FP16 \u00b6 Certain GPUs, notably the cheap T4 and the expensive V100, support the ability to process models using FP16, giving massive speed and memory improvements, Assuming you are using a compatable GPU and already have apex installed, you can convert a model to the \"half\" FP16 mode with this: ai . to_fp16 () If you want to convert the model before loading it into GPU memory (which may help avoid memory leaks), you can instantiate the model like this: ai . to_fp16 ( to_gpu = True , to_fp16 = True ) With this, you can generate massive amounts of text from even the GPT-2 1.5B model!","title":"Improving Generation Performance"},{"location":"generate-performance/#improving-generation-performance","text":"A few tips and tricks for improving generation performance for both on CPUs and GPUs. (note that with these tricks, you cannot train the model afterwards!)","title":"Improving Generation Performance"},{"location":"generate-performance/#cpu","text":"","title":"CPU"},{"location":"generate-performance/#quantization","text":"PyTorch has the ability to quantize models on the CPU. Currently, it will only quantize the Linear layer of GPT-2, but the generation performances increases 15% \u2014 25% ; far from trivial! To quantize a model after it's loaded, just run: ai . quantize ()","title":"Quantization"},{"location":"generate-performance/#gpu","text":"","title":"GPU"},{"location":"generate-performance/#fp16","text":"Certain GPUs, notably the cheap T4 and the expensive V100, support the ability to process models using FP16, giving massive speed and memory improvements, Assuming you are using a compatable GPU and already have apex installed, you can convert a model to the \"half\" FP16 mode with this: ai . to_fp16 () If you want to convert the model before loading it into GPU memory (which may help avoid memory leaks), you can instantiate the model like this: ai . to_fp16 ( to_gpu = True , to_fp16 = True ) With this, you can generate massive amounts of text from even the GPT-2 1.5B model!","title":"FP16"},{"location":"generate/","text":"Text Generation \u00b6 Thanks to the base Transformers package, aitextgen has more options for generating text than other text-generating apps before. Generation Parameters \u00b6 See this article by Huggingface engineer Patrick von Platen for how sampling and these parameters are used in practice. n : Number of texts generated. max_length : Maximum length of the generated text (default: 200; for GPT-2, the maximum is 1024; for GPT Neo, the maximum is 2048) prompt : Prompt that starts the generated text and is included in the generated text. temperature : Controls the \"craziness\" of the text (default: 0.7) top_k : If nonzero, limits the sampled tokens to the top k values. (default: 0) top_p : If nonzero, limits the sampled tokens to the cumulative probability Some lesser-known-but-still-useful-parameters that are unique to Transformers: Performance Enabling these parameters may slow down generation. num_beams : If greater than 1, executes beam search for cleaner text. repetition_penalty : If greater than 1.0, penalizes repetition in a text to avoid infinite loops. length_penalty : If greater than 1.0, penalizes text proportional to the length no_repeat_ngram_size : Token length to avoid repeating given phrases. Generation Functions \u00b6 Given a aitextgen object with a loaded model + tokenizer named ai : About devices aitextgen does not automatically set the device used to generate text. If you want to generate on the GPU, make sure you call ai.to_gpu() beforehand, or load the model into the GPU using ai = aitextgen(to_gpu=True) ai.generate() : Generates and prints text to console. If prompt is used, the prompt is bolded . ai.generate_one() : A helper function which generates a single text and returns as a string (good for APIs) ai.generate_samples() : Generates multiple samples at specified temperatures: great for debugging. ai.generate_to_file() : Generates a bulk amount of texts to file. (this accepts a batch_size parameter which is useful if using on a GPU, as it can generate texts in parallel with no performance loss) lstrip and nonempty_output By default, the lstrip and nonempty_output parameters to generate are set to True , which alters the behavior of the generated text in a way that is most likely preferable. lstrip : Removes all whitespace at the beginning of the generated space. nonempty_output : If the output is empty (possible on shortform content), skip it if generating multiple texts, or try again if it's a single text. If min_length is specified, the same behavior occurs for texts below the minimum length after processing. Seed \u00b6 aitextgen has a new seed parameter for generation. Using any generate function with a seed parameter (must be an integer) and all other models/parameters the same, and the generated text will be identical. This allows for reproducible generations in case someone accuses you of faking the AI output. For generate_to_file() , the 8-digit number at the end of the file name will be the seed used to generate the file, making reprodicibility easy.","title":"Generating Text"},{"location":"generate/#text-generation","text":"Thanks to the base Transformers package, aitextgen has more options for generating text than other text-generating apps before.","title":"Text Generation"},{"location":"generate/#generation-parameters","text":"See this article by Huggingface engineer Patrick von Platen for how sampling and these parameters are used in practice. n : Number of texts generated. max_length : Maximum length of the generated text (default: 200; for GPT-2, the maximum is 1024; for GPT Neo, the maximum is 2048) prompt : Prompt that starts the generated text and is included in the generated text. temperature : Controls the \"craziness\" of the text (default: 0.7) top_k : If nonzero, limits the sampled tokens to the top k values. (default: 0) top_p : If nonzero, limits the sampled tokens to the cumulative probability Some lesser-known-but-still-useful-parameters that are unique to Transformers: Performance Enabling these parameters may slow down generation. num_beams : If greater than 1, executes beam search for cleaner text. repetition_penalty : If greater than 1.0, penalizes repetition in a text to avoid infinite loops. length_penalty : If greater than 1.0, penalizes text proportional to the length no_repeat_ngram_size : Token length to avoid repeating given phrases.","title":"Generation Parameters"},{"location":"generate/#generation-functions","text":"Given a aitextgen object with a loaded model + tokenizer named ai : About devices aitextgen does not automatically set the device used to generate text. If you want to generate on the GPU, make sure you call ai.to_gpu() beforehand, or load the model into the GPU using ai = aitextgen(to_gpu=True) ai.generate() : Generates and prints text to console. If prompt is used, the prompt is bolded . ai.generate_one() : A helper function which generates a single text and returns as a string (good for APIs) ai.generate_samples() : Generates multiple samples at specified temperatures: great for debugging. ai.generate_to_file() : Generates a bulk amount of texts to file. (this accepts a batch_size parameter which is useful if using on a GPU, as it can generate texts in parallel with no performance loss) lstrip and nonempty_output By default, the lstrip and nonempty_output parameters to generate are set to True , which alters the behavior of the generated text in a way that is most likely preferable. lstrip : Removes all whitespace at the beginning of the generated space. nonempty_output : If the output is empty (possible on shortform content), skip it if generating multiple texts, or try again if it's a single text. If min_length is specified, the same behavior occurs for texts below the minimum length after processing.","title":"Generation Functions"},{"location":"generate/#seed","text":"aitextgen has a new seed parameter for generation. Using any generate function with a seed parameter (must be an integer) and all other models/parameters the same, and the generated text will be identical. This allows for reproducible generations in case someone accuses you of faking the AI output. For generate_to_file() , the 8-digit number at the end of the file name will be the seed used to generate the file, making reprodicibility easy.","title":"Seed"},{"location":"gpt-2-simple/","text":"Importing from gpt-2-simple \u00b6 Want to import a model trained using gpt-2-simple , or another GPT-2 based finetuning approach? You can do that using the transformers-cli . In the case of gpt-2-simple (where the output is structured checkpoint/run1 ), you'd cd into the directory containing the checkpoint folder and run: transformers-cli convert --model_type gpt2 --tf_checkpoint checkpoint/run1 --pytorch_dump_output pytorch --config checkpoint/run1/hparams.json This will put a pytorch_model.bin and config.json in the pytorch folder, which is what you'll need to pass to aitextgen() to load the model. That's it!","title":"Importing from gpt-2-simple"},{"location":"gpt-2-simple/#importing-from-gpt-2-simple","text":"Want to import a model trained using gpt-2-simple , or another GPT-2 based finetuning approach? You can do that using the transformers-cli . In the case of gpt-2-simple (where the output is structured checkpoint/run1 ), you'd cd into the directory containing the checkpoint folder and run: transformers-cli convert --model_type gpt2 --tf_checkpoint checkpoint/run1 --pytorch_dump_output pytorch --config checkpoint/run1/hparams.json This will put a pytorch_model.bin and config.json in the pytorch folder, which is what you'll need to pass to aitextgen() to load the model. That's it!","title":"Importing from gpt-2-simple"},{"location":"helpful-notes/","text":"Helpful Notes \u00b6 A few helpful tips and tricks for using aitextgen. Not all AI generated text will be good , hence why human curation is currently a necessary strategy for many finetuned models. In testing, only 5% \u2014 10% of generated text is viable. One of the design goals of aitextgen is to help provide tools to improve that signal-to-noise ratio. You may not necessarily get better results with larger models . Larger models perform better on academic benchmarks, yes, but the quality of text can vary strongly depending on the size of the model used, especially if you do not have a lot of input data. To convert a GPT-2 model trained using earlier TensorFlow-based finetuning tools such as gpt-2-simple to the PyTorch format, use the transformers-cli command and the instructions here to convert the checkpoint (where OPENAI_GPT2_CHECKPOINT_PATH is the folder containing the model) When running on Google Cloud Platform (including Google Colab), it's recommended to download the TF-based GPT-2 from the Google API vs. downloading the PyTorch GPT-2 from Huggingface as the download will be much faster and also saves Huggingface some bandwidth. If you want to generate text from a GPU, you must manually move the model to the GPU (it will not be done automatically to save GPU VRAM for training). Either call to_gpu=True when loading the model or call to_gpu() from the aitextgen object. Encoding your text dataset before moving it to a cloud/remote server is strongly recommended. You can do that quickly from the CLI ( aitextgen encode text.txt ) Thanks to a few tricks, the file size is reduced by about 1/2 to 2/3, and the encoded text will instantly load on the remote server! If you're making a micro-GPT-2 model, using a GPU with a large batch size is recommended, as with the AdamW optimizer, it effectively has built-in batch normalization. If you make frequent use of the Colab Notebooks, I recommend purchasing Colab Pro : the timeouts are infrequent, and being able to reliably train on a P100 (normally $1.46/hr ) for hours on end very quickly pays for itself!","title":"Helpful Notes"},{"location":"helpful-notes/#helpful-notes","text":"A few helpful tips and tricks for using aitextgen. Not all AI generated text will be good , hence why human curation is currently a necessary strategy for many finetuned models. In testing, only 5% \u2014 10% of generated text is viable. One of the design goals of aitextgen is to help provide tools to improve that signal-to-noise ratio. You may not necessarily get better results with larger models . Larger models perform better on academic benchmarks, yes, but the quality of text can vary strongly depending on the size of the model used, especially if you do not have a lot of input data. To convert a GPT-2 model trained using earlier TensorFlow-based finetuning tools such as gpt-2-simple to the PyTorch format, use the transformers-cli command and the instructions here to convert the checkpoint (where OPENAI_GPT2_CHECKPOINT_PATH is the folder containing the model) When running on Google Cloud Platform (including Google Colab), it's recommended to download the TF-based GPT-2 from the Google API vs. downloading the PyTorch GPT-2 from Huggingface as the download will be much faster and also saves Huggingface some bandwidth. If you want to generate text from a GPU, you must manually move the model to the GPU (it will not be done automatically to save GPU VRAM for training). Either call to_gpu=True when loading the model or call to_gpu() from the aitextgen object. Encoding your text dataset before moving it to a cloud/remote server is strongly recommended. You can do that quickly from the CLI ( aitextgen encode text.txt ) Thanks to a few tricks, the file size is reduced by about 1/2 to 2/3, and the encoded text will instantly load on the remote server! If you're making a micro-GPT-2 model, using a GPU with a large batch size is recommended, as with the AdamW optimizer, it effectively has built-in batch normalization. If you make frequent use of the Colab Notebooks, I recommend purchasing Colab Pro : the timeouts are infrequent, and being able to reliably train on a P100 (normally $1.46/hr ) for hours on end very quickly pays for itself!","title":"Helpful Notes"},{"location":"load-model/","text":"Model Loading \u00b6 There are several ways to load models. Continue Training/Finetuning You can further train a model by reloading a model that has already been trained, using the methods outlined below. Loading an aitextgen model \u00b6 For the base case, loading the default 124M GPT-2 model via Huggingface: ai = aitextgen () The downloaded model will be downloaded to cache_dir : /aitextgen by default. If you're loading a custom model for a different GPT-2/GPT-Neo architecture from scratch but with the normal GPT-2 tokenizer, you can pass only a config. from aitextgen.utils import GPT2ConfigCPU config = GPT2ConfigCPU () ai = aitextgen ( config = config ) While training/finetuning a model, two files will be created: the pytorch_model.bin which contains the weights for the model, and a config.json illustrating the architecture for the model. Both of these files are needed to reload the model. If you've finetuned a model using aitextgen (the default model), you can pass the folder name containing the generated pytorch_model.bin and config.json to aitextgen (e.g. trained_model , which is where trained models will be saved by default). Same Directory If both files are in the current directory, you can pass model_folder=\".\" . ai = aitextgen ( model_folder = \"trained_model\" ) These examples assume you are using the default GPT-2 tokenizer. If you have a custom tokenizer , you'll need to pass that along with loading the model. ai3 = aitextgen ( model_folder = \"trained_model\" , tokenizer_file = \"aitextgen.tokenizer.json\" ) If you want to download an alternative GPT-2 model from Hugging Face's repository of models, pass that model name to model . ai = aitextgen ( model = \"minimaxir/hacker-news\" ) The model and associated config + tokenizer will be downloaded into cache_dir . This can also be used to download the pretrained GPT Neo models from EleutherAI. ai = aitextgen ( model = \"EleutherAI/gpt-neo-125M\" ) Loading TensorFlow-based GPT-2 models \u00b6 aitextgen lets you download the models from Microsoft's servers that OpenAI had uploaded back when GPT-2 was first released in 2019. These models are then converted to a PyTorch format. To use this workflow, pass the corresponding model number to tf_gpt2 : ai = aitextgen ( tf_gpt2 = \"124M\" ) This will cache the converted model locally in cache_dir , and using the same parameters will load the converted model. The valid TF model names are [\"124M\",\"355M\",\"774M\",\"1558M\"] .","title":"Loading a Model"},{"location":"load-model/#model-loading","text":"There are several ways to load models. Continue Training/Finetuning You can further train a model by reloading a model that has already been trained, using the methods outlined below.","title":"Model Loading"},{"location":"load-model/#loading-an-aitextgen-model","text":"For the base case, loading the default 124M GPT-2 model via Huggingface: ai = aitextgen () The downloaded model will be downloaded to cache_dir : /aitextgen by default. If you're loading a custom model for a different GPT-2/GPT-Neo architecture from scratch but with the normal GPT-2 tokenizer, you can pass only a config. from aitextgen.utils import GPT2ConfigCPU config = GPT2ConfigCPU () ai = aitextgen ( config = config ) While training/finetuning a model, two files will be created: the pytorch_model.bin which contains the weights for the model, and a config.json illustrating the architecture for the model. Both of these files are needed to reload the model. If you've finetuned a model using aitextgen (the default model), you can pass the folder name containing the generated pytorch_model.bin and config.json to aitextgen (e.g. trained_model , which is where trained models will be saved by default). Same Directory If both files are in the current directory, you can pass model_folder=\".\" . ai = aitextgen ( model_folder = \"trained_model\" ) These examples assume you are using the default GPT-2 tokenizer. If you have a custom tokenizer , you'll need to pass that along with loading the model. ai3 = aitextgen ( model_folder = \"trained_model\" , tokenizer_file = \"aitextgen.tokenizer.json\" ) If you want to download an alternative GPT-2 model from Hugging Face's repository of models, pass that model name to model . ai = aitextgen ( model = \"minimaxir/hacker-news\" ) The model and associated config + tokenizer will be downloaded into cache_dir . This can also be used to download the pretrained GPT Neo models from EleutherAI. ai = aitextgen ( model = \"EleutherAI/gpt-neo-125M\" )","title":"Loading an aitextgen model"},{"location":"load-model/#loading-tensorflow-based-gpt-2-models","text":"aitextgen lets you download the models from Microsoft's servers that OpenAI had uploaded back when GPT-2 was first released in 2019. These models are then converted to a PyTorch format. To use this workflow, pass the corresponding model number to tf_gpt2 : ai = aitextgen ( tf_gpt2 = \"124M\" ) This will cache the converted model locally in cache_dir , and using the same parameters will load the converted model. The valid TF model names are [\"124M\",\"355M\",\"774M\",\"1558M\"] .","title":"Loading TensorFlow-based GPT-2 models"},{"location":"loggers/","text":"Loggers \u00b6 You can create loggers with popular tools such as TensorBoard and Weights and Biases by leveraging pytorch-lightning's logger functionality. See their documentation on all the available options for loggers. For example, if you want to create a TensorBoard logger, you can create it: from pytorch_lightning import loggers tb_logger = loggers . TensorBoardLogger ( 'logs/' ) Then pass it to the loggers parameter for ai.train() . ai . train ( train_data = data , loggers = tb_logger )","title":"Loggers"},{"location":"loggers/#loggers","text":"You can create loggers with popular tools such as TensorBoard and Weights and Biases by leveraging pytorch-lightning's logger functionality. See their documentation on all the available options for loggers. For example, if you want to create a TensorBoard logger, you can create it: from pytorch_lightning import loggers tb_logger = loggers . TensorBoardLogger ( 'logs/' ) Then pass it to the loggers parameter for ai.train() . ai . train ( train_data = data , loggers = tb_logger )","title":"Loggers"},{"location":"save-model/","text":"Model Saving \u00b6 There are are multiple ways to save models. Whenever a model is saved, two files are generated: pytorch_model.bin which contains the model weights, and config.json which is needed to load the model. Assuming we have an aitextgen model ai : Ad Hoc saving \u00b6 The aitextgen model can be saved at any time using save . ai . save () Save to Google Drive \u00b6 If you are using Google Colaboratory, you can mount your personal Google Drive to the notebook and save your models there. Downloading models from Colab Notebooks It's strongly recommended to move models to Google Drive before downloading them from Colaboratory. First mount your Google Drive using mount_gdrive() : from aitextgen.colab import mount_gdrive , copy_file_to_gdrive mount_gdrive () You'll be asked for an auth code; input it and press enter, and a My Drive folder will appear in Colab Files view. You can drag and drop the model files into the Google Drive, or use copy_file_to_gdrive to copy them programmatically. copy_file_to_gdrive ( \"pytorch_model.bin\" ) copy_file_to_gdrive ( \"config.json\" ) Saving During Training \u00b6 By default, the train() function has save_every = 1000 , which means the model will save every 1000 steps to the specified output_dir ( trained_model by default). You can adjust as necessary. Saving During Training in Google Colab \u00b6 Concerned about timeouts in Google Colab? aitextgen has a feature that will copy models to your Google Drive periodically in case the instance gets killed! As long as your drive is mounted as above, pass save_gdrive = True to the train() function: ai . train ( save_gdrive = True ) This will save the model to the folder corresponding to the training run_id parameter (the datetime training was called, to prevent accidently overwriting).","title":"Saving a Model"},{"location":"save-model/#model-saving","text":"There are are multiple ways to save models. Whenever a model is saved, two files are generated: pytorch_model.bin which contains the model weights, and config.json which is needed to load the model. Assuming we have an aitextgen model ai :","title":"Model Saving"},{"location":"save-model/#ad-hoc-saving","text":"The aitextgen model can be saved at any time using save . ai . save ()","title":"Ad Hoc saving"},{"location":"save-model/#save-to-google-drive","text":"If you are using Google Colaboratory, you can mount your personal Google Drive to the notebook and save your models there. Downloading models from Colab Notebooks It's strongly recommended to move models to Google Drive before downloading them from Colaboratory. First mount your Google Drive using mount_gdrive() : from aitextgen.colab import mount_gdrive , copy_file_to_gdrive mount_gdrive () You'll be asked for an auth code; input it and press enter, and a My Drive folder will appear in Colab Files view. You can drag and drop the model files into the Google Drive, or use copy_file_to_gdrive to copy them programmatically. copy_file_to_gdrive ( \"pytorch_model.bin\" ) copy_file_to_gdrive ( \"config.json\" )","title":"Save to Google Drive"},{"location":"save-model/#saving-during-training","text":"By default, the train() function has save_every = 1000 , which means the model will save every 1000 steps to the specified output_dir ( trained_model by default). You can adjust as necessary.","title":"Saving During Training"},{"location":"save-model/#saving-during-training-in-google-colab","text":"Concerned about timeouts in Google Colab? aitextgen has a feature that will copy models to your Google Drive periodically in case the instance gets killed! As long as your drive is mounted as above, pass save_gdrive = True to the train() function: ai . train ( save_gdrive = True ) This will save the model to the folder corresponding to the training run_id parameter (the datetime training was called, to prevent accidently overwriting).","title":"Saving During Training in Google Colab"},{"location":"upload/","text":"Upload Model to Huggingface \u00b6 You can upload your trained models to Huggingface, where it can be downloaded by others! To upload your model, you'll have to create a folder which has 6 files: pytorch_model.bin config.json vocab.json merges.txt special_tokens_map.json tokenizer_config.json You can generate all of these files at the same time into a given folder by running ai.save_for_upload(model_name) . Then, follow the transformers-cli instructions to upload the model. transformers-cli login transformers-cli upload model_name You (or another user) can download cache, and generate from that model via: ai = aitextgen(model=\"username/model_name\")","title":"Upload Model to Huggingface"},{"location":"upload/#upload-model-to-huggingface","text":"You can upload your trained models to Huggingface, where it can be downloaded by others! To upload your model, you'll have to create a folder which has 6 files: pytorch_model.bin config.json vocab.json merges.txt special_tokens_map.json tokenizer_config.json You can generate all of these files at the same time into a given folder by running ai.save_for_upload(model_name) . Then, follow the transformers-cli instructions to upload the model. transformers-cli login transformers-cli upload model_name You (or another user) can download cache, and generate from that model via: ai = aitextgen(model=\"username/model_name\")","title":"Upload Model to Huggingface"},{"location":"tutorials/colab/","text":"Colaboratory Notebooks \u00b6 You cannot finetune OpenAI's GPT-2 models on CPU (and not even on some consumer GPUs). Therefore, there are a couple Google Colaboratory notebooks, which provide a GPU suitable for finetuning a model. The Colab Notebooks also contain utilities to make it easier to export the model to Google Drive during and after training. Finetuning OpenAI's Model \u00b6 Colab Notebook A Notebook for finetuning OpenAI's model on a GPU. This is the most common use case. Note Currently you can only finetune the 124M/355M/774M OpenAI GPT-2 models, with the latter two forcing gradient_checkpointing=True to ensure it does not cause the Colab GPU to go OOM. Training Your Own GPT-2 Model \u00b6 Colab Notebook A Notebook for creating your own GPT-2 model with your own tokenizer. See the Model From Scratch section on the advantages and disadvantages of this approach.","title":"Colaboratory Notebooks"},{"location":"tutorials/colab/#colaboratory-notebooks","text":"You cannot finetune OpenAI's GPT-2 models on CPU (and not even on some consumer GPUs). Therefore, there are a couple Google Colaboratory notebooks, which provide a GPU suitable for finetuning a model. The Colab Notebooks also contain utilities to make it easier to export the model to Google Drive during and after training.","title":"Colaboratory Notebooks"},{"location":"tutorials/colab/#finetuning-openais-model","text":"Colab Notebook A Notebook for finetuning OpenAI's model on a GPU. This is the most common use case. Note Currently you can only finetune the 124M/355M/774M OpenAI GPT-2 models, with the latter two forcing gradient_checkpointing=True to ensure it does not cause the Colab GPU to go OOM.","title":"Finetuning OpenAI's Model"},{"location":"tutorials/colab/#training-your-own-gpt-2-model","text":"Colab Notebook A Notebook for creating your own GPT-2 model with your own tokenizer. See the Model From Scratch section on the advantages and disadvantages of this approach.","title":"Training Your Own GPT-2 Model"},{"location":"tutorials/generate_1_5b/","text":"Generating From GPT-2 1.5B \u00b6 Want to generate a ton of text with the largest GPT-2 model, with the generation control provided by aitextgen? Now you can, at a surprisingly low cost! ($0.382/hr, prorated to the nearest second) Here's how to set it up on Google Cloud Platform. Setting Up an AI Platform Notebook \u00b6 An AI Platform Notebook is a hosted Jupyter Lab instance on Google Cloud Platform oriented for AI training and inference. Since it requires zero setup and has no additional costs outside of CPU/GPUs , it's the best tool to play with aitextgen. First, go to AI Platform Notebooks in the GCP console (if you haven't made a project + billing, it should prompt you to do so). Go to New Instance , select PyTorch 1.4 and With 1 NVIDIA Tesla T4 . Quotas You may need T4 quota to create a VM with a T4; accounts should have enough by default, but you may want to confirm. The rest of the VM config settings are fine to leave as/is, however make sure you check Install NVIDIA GPU driver automatically for me ! Once the instance is created, wait a bit (it takes awhile to install the driver), and a OPEN JUPYTERLAB button will appear. Click it to open the hosted Jupyter Lab Installing aitextgen on the VM \u00b6 Now we have to install the dependencies, which only have to be done once. In the Jupyter Lab instance, open a Terminal tab, and install both aitextgen and tensorflow (we'll need tensorflow later) pip3 install aitextgen tensorflow Now the harder part: we need to install and compile apex for FP16 support with the T4 GPU. To do that, run: git clone https://github.com/NVIDIA/apex cd apex && pip install -v --no-cache-dir --global-option = \"--cpp_ext\" --global-option = \"--cuda_ext\" ./ That will take a few minutes, but once that is done, you are good to go and do not need to rerun these steps again! Loading GPT-2 1.5B \u00b6 Now go back to the Launcher and create a Python 3 Notebook (or upload the one here). CUDA You may want to ensure the Notebook sees the CUDA installation, which appears to be somewhat random. This can be verified by running import torch in a cell, then torch.cuda.is_available() . In a cell, load aitextgen: from aitextgen import aitextgen In another cell, input and run: ai = aitextgen ( tf_gpt2 = \"1558M\" , to_gpu = True , to_fp16 = True ) A few things going on here: The TensorFlow-based GPT-2 1.5B is downloaded from Google's servers. (download rate is very fast). This download will only occur once. It is converted to a corresponding PyTorch model, and then loaded. After it is loaded, it is converted to a FP16 representation. Then it is moved to the T4 GPU. Generating from GPT-2 1.5B \u00b6 Now we can generate texts! The T4, for GPT-2 1.5B in FP16 mode, can generate about 30 texts in a batch without going OOM. (you can verify GPU memory usage at any time by opening up a Terminal and running nvidia-smi ) Create a cell and add: ai . generate_to_file ( n = 300 , batch_size = 30 ) Batch Size The batch size of 30 above assumes the default max_length of 256. If you want to use the full 1024 token max length, lower the batch size to 15, as the GPU will go OOM otherwise. And it will generate the texts to a file! When completed, you can double-click to view it in Jupyter Lab, and you can download the file by right-clicking it from the file viewer. More importantly, all parameters to generate are valid, allowing massive flexibility! ai . generate_to_file ( n = 150 , batch_size = 15 , max_length = 1024 , top_p = 0.9 , temperature = 1.2 , prompt = \"President Donald Trump has magically transformed into a unicorn.\" ) Cleanup \u00b6 Make sure you Stop the instance when you are done to avoid being charged . To do that, go back to the AI Platform Notebook console, select the instance, and press Stop. Since 100GB of storage may be pricy, you may want to consider deleting the VM fully if you are done with it as well.","title":"Generating From GPT-2 1.5B"},{"location":"tutorials/generate_1_5b/#generating-from-gpt-2-15b","text":"Want to generate a ton of text with the largest GPT-2 model, with the generation control provided by aitextgen? Now you can, at a surprisingly low cost! ($0.382/hr, prorated to the nearest second) Here's how to set it up on Google Cloud Platform.","title":"Generating From GPT-2 1.5B"},{"location":"tutorials/generate_1_5b/#setting-up-an-ai-platform-notebook","text":"An AI Platform Notebook is a hosted Jupyter Lab instance on Google Cloud Platform oriented for AI training and inference. Since it requires zero setup and has no additional costs outside of CPU/GPUs , it's the best tool to play with aitextgen. First, go to AI Platform Notebooks in the GCP console (if you haven't made a project + billing, it should prompt you to do so). Go to New Instance , select PyTorch 1.4 and With 1 NVIDIA Tesla T4 . Quotas You may need T4 quota to create a VM with a T4; accounts should have enough by default, but you may want to confirm. The rest of the VM config settings are fine to leave as/is, however make sure you check Install NVIDIA GPU driver automatically for me ! Once the instance is created, wait a bit (it takes awhile to install the driver), and a OPEN JUPYTERLAB button will appear. Click it to open the hosted Jupyter Lab","title":"Setting Up an AI Platform Notebook"},{"location":"tutorials/generate_1_5b/#installing-aitextgen-on-the-vm","text":"Now we have to install the dependencies, which only have to be done once. In the Jupyter Lab instance, open a Terminal tab, and install both aitextgen and tensorflow (we'll need tensorflow later) pip3 install aitextgen tensorflow Now the harder part: we need to install and compile apex for FP16 support with the T4 GPU. To do that, run: git clone https://github.com/NVIDIA/apex cd apex && pip install -v --no-cache-dir --global-option = \"--cpp_ext\" --global-option = \"--cuda_ext\" ./ That will take a few minutes, but once that is done, you are good to go and do not need to rerun these steps again!","title":"Installing aitextgen on the VM"},{"location":"tutorials/generate_1_5b/#loading-gpt-2-15b","text":"Now go back to the Launcher and create a Python 3 Notebook (or upload the one here). CUDA You may want to ensure the Notebook sees the CUDA installation, which appears to be somewhat random. This can be verified by running import torch in a cell, then torch.cuda.is_available() . In a cell, load aitextgen: from aitextgen import aitextgen In another cell, input and run: ai = aitextgen ( tf_gpt2 = \"1558M\" , to_gpu = True , to_fp16 = True ) A few things going on here: The TensorFlow-based GPT-2 1.5B is downloaded from Google's servers. (download rate is very fast). This download will only occur once. It is converted to a corresponding PyTorch model, and then loaded. After it is loaded, it is converted to a FP16 representation. Then it is moved to the T4 GPU.","title":"Loading GPT-2 1.5B"},{"location":"tutorials/generate_1_5b/#generating-from-gpt-2-15b_1","text":"Now we can generate texts! The T4, for GPT-2 1.5B in FP16 mode, can generate about 30 texts in a batch without going OOM. (you can verify GPU memory usage at any time by opening up a Terminal and running nvidia-smi ) Create a cell and add: ai . generate_to_file ( n = 300 , batch_size = 30 ) Batch Size The batch size of 30 above assumes the default max_length of 256. If you want to use the full 1024 token max length, lower the batch size to 15, as the GPU will go OOM otherwise. And it will generate the texts to a file! When completed, you can double-click to view it in Jupyter Lab, and you can download the file by right-clicking it from the file viewer. More importantly, all parameters to generate are valid, allowing massive flexibility! ai . generate_to_file ( n = 150 , batch_size = 15 , max_length = 1024 , top_p = 0.9 , temperature = 1.2 , prompt = \"President Donald Trump has magically transformed into a unicorn.\" )","title":"Generating from GPT-2 1.5B"},{"location":"tutorials/generate_1_5b/#cleanup","text":"Make sure you Stop the instance when you are done to avoid being charged . To do that, go back to the AI Platform Notebook console, select the instance, and press Stop. Since 100GB of storage may be pricy, you may want to consider deleting the VM fully if you are done with it as well.","title":"Cleanup"},{"location":"tutorials/hello-world/","text":"Hello World \u00b6 Here's how you can quickly test out aitextgen on your own computer, even if you don't have a GPU! For generating text from a pretrained GPT-2 model: from aitextgen import aitextgen # Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model ai = aitextgen () ai . generate () ai . generate ( n = 3 , max_length = 100 ) ai . generate ( n = 3 , prompt = \"I believe in unicorns because\" , max_length = 100 ) ai . generate_to_file ( n = 10 , prompt = \"I believe in unicorns because\" , max_length = 100 , temperature = 1.2 ) You can also generate from the command line: aitextgen generate aitextgen generate --prompt \"I believe in unicorns because\" --to_file False Want to train your own mini GPT-2 model on your own computer? Download this text file of Shakespeare plays , cd to that directory in a Teriminal, open up a python3 console and go: from aitextgen.TokenDataset import TokenDataset from aitextgen.tokenizers import train_tokenizer from aitextgen.utils import GPT2ConfigCPU from aitextgen import aitextgen # The name of the downloaded Shakespeare text for training file_name = \"input.txt\" # Train a custom BPE Tokenizer on the downloaded text # This will save one file: `aitextgen.tokenizer.json`, which contains the # information needed to rebuild the tokenizer. train_tokenizer ( file_name ) tokenizer_file = \"aitextgen.tokenizer.json\" # GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training # e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2. config = GPT2ConfigCPU () # Instantiate aitextgen using the created tokenizer and config ai = aitextgen ( tokenizer_file = tokenizer_file , config = config ) # You can build datasets for training by creating TokenDatasets, # which automatically processes the dataset with the appropriate size. data = TokenDataset ( file_name , tokenizer_file = tokenizer_file , block_size = 64 ) # Train the model! It will save pytorch_model.bin periodically and after completion to the `trained_model` folder. # On a 2020 8-core iMac, this took ~25 minutes to run. ai . train ( data , batch_size = 8 , num_steps = 50000 , generate_every = 5000 , save_every = 5000 ) # Generate text from it! ai . generate ( 10 , prompt = \"ROMEO:\" ) # With your trained model, you can reload the model at any time by # providing the folder containing the pytorch_model.bin model weights + the config, and providing the tokenizer. ai2 = aitextgen ( model_folder = \"trained_model\" , tokenizer_file = \"aitextgen.tokenizer.json\" ) ai2 . generate ( 10 , prompt = \"ROMEO:\" )","title":"Hello World Tutorial"},{"location":"tutorials/hello-world/#hello-world","text":"Here's how you can quickly test out aitextgen on your own computer, even if you don't have a GPU! For generating text from a pretrained GPT-2 model: from aitextgen import aitextgen # Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model ai = aitextgen () ai . generate () ai . generate ( n = 3 , max_length = 100 ) ai . generate ( n = 3 , prompt = \"I believe in unicorns because\" , max_length = 100 ) ai . generate_to_file ( n = 10 , prompt = \"I believe in unicorns because\" , max_length = 100 , temperature = 1.2 ) You can also generate from the command line: aitextgen generate aitextgen generate --prompt \"I believe in unicorns because\" --to_file False Want to train your own mini GPT-2 model on your own computer? Download this text file of Shakespeare plays , cd to that directory in a Teriminal, open up a python3 console and go: from aitextgen.TokenDataset import TokenDataset from aitextgen.tokenizers import train_tokenizer from aitextgen.utils import GPT2ConfigCPU from aitextgen import aitextgen # The name of the downloaded Shakespeare text for training file_name = \"input.txt\" # Train a custom BPE Tokenizer on the downloaded text # This will save one file: `aitextgen.tokenizer.json`, which contains the # information needed to rebuild the tokenizer. train_tokenizer ( file_name ) tokenizer_file = \"aitextgen.tokenizer.json\" # GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training # e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2. config = GPT2ConfigCPU () # Instantiate aitextgen using the created tokenizer and config ai = aitextgen ( tokenizer_file = tokenizer_file , config = config ) # You can build datasets for training by creating TokenDatasets, # which automatically processes the dataset with the appropriate size. data = TokenDataset ( file_name , tokenizer_file = tokenizer_file , block_size = 64 ) # Train the model! It will save pytorch_model.bin periodically and after completion to the `trained_model` folder. # On a 2020 8-core iMac, this took ~25 minutes to run. ai . train ( data , batch_size = 8 , num_steps = 50000 , generate_every = 5000 , save_every = 5000 ) # Generate text from it! ai . generate ( 10 , prompt = \"ROMEO:\" ) # With your trained model, you can reload the model at any time by # providing the folder containing the pytorch_model.bin model weights + the config, and providing the tokenizer. ai2 = aitextgen ( model_folder = \"trained_model\" , tokenizer_file = \"aitextgen.tokenizer.json\" ) ai2 . generate ( 10 , prompt = \"ROMEO:\" )","title":"Hello World"},{"location":"tutorials/model-from-scratch/","text":"Training a GPT-2 Model From Scratch \u00b6 The original GPT-2 model released by OpenAI was trained on English webpages linked to from Reddit, with a strong bias toward longform content (multiple paragraphs). If that is not your use case, you may get a better generation quality and speed by training your own model and Tokenizer. Examples of good use cases: Short-form content (e.g. Tweets, Reddit post titles) Non-English Text Heavily Encoded Text It still will require a massive amount of training time (several hours) but will be more flexible. Building a Custom Tokenizer. \u00b6 The train_tokenizer() function from aitextgen.tokenizers trains the model on the specified text(s) on disk. Vocabulary Size The default vocabulary size for train_tokenizer() is 1,000 tokens. Although this is much lower than GPT-2's 50k vocab size, the smaller the vocab size, the easier it is to train the model (since it's more likely for the model to make a correct \"guess\"), and the model file size will be much smaller. from aitextgen.tokenizers import train_tokenizer train_tokenizer ( file_name ) This creates one file, aitextgen.tokenizer.json , which is needed to rebuild the tokenizer. Building a Custom Dataset \u00b6 You can build a TokenDataset based off your custom Tokenizer, to be fed into the model. data = TokenDataset ( file_name , vocab_file = vocab_file , merges_file = merges_file , block_size = 32 ) Building a Custom Config \u00b6 Whenever you load a default 124M GPT-2 model, it uses a GPT2Config() under the hood. But you can create your own, with whatever parameters you want. The build_gpt2_config() function from aitextgen.utils gives you more control. config = build_gpt2_config ( vocab_size = 5000 , max_length = 32 , dropout = 0.0 , n_embd = 256 , n_layer = 8 , n_head = 8 ) A few notes on the inputs: vocab_size : Vocabulary size: this must match what you used to build the tokenizer! max_length : Context window for the GPT-2 model: this must match the block_size used in the TokenDataset! dropout : Dropout on various areas of the model to limit overfitting (you should likely keep at 0) n_embd : The embedding size for each vocab token. n_layers : Transformer layers n_head : Transformer heads Model Size GPT-2 Model size is directly proportional to vocab_size * embeddings . Training the Custom Model \u00b6 You can instantiate an empty GPT-2 according to your custom config, and construct a custom tokenizer according to your vocab and merges file: ai = aitextgen ( tokenizer_file = tokenizer_file , config = config ) Training is done as normal. ai . train ( data , batch_size = 16 , num_steps = 5000 ) Reloading the Custom Model \u00b6 You'll always need to provide the tokenizer_file and the folder containing the pytorch_model.bin and config.json . ai = aitextgen ( model_folder = \"trained_model\" , tokenizer_file = \"aitextgen.tokenizer.json\" )","title":"Training a GPT-2 Model From Scratch"},{"location":"tutorials/model-from-scratch/#training-a-gpt-2-model-from-scratch","text":"The original GPT-2 model released by OpenAI was trained on English webpages linked to from Reddit, with a strong bias toward longform content (multiple paragraphs). If that is not your use case, you may get a better generation quality and speed by training your own model and Tokenizer. Examples of good use cases: Short-form content (e.g. Tweets, Reddit post titles) Non-English Text Heavily Encoded Text It still will require a massive amount of training time (several hours) but will be more flexible.","title":"Training a GPT-2 Model From Scratch"},{"location":"tutorials/model-from-scratch/#building-a-custom-tokenizer","text":"The train_tokenizer() function from aitextgen.tokenizers trains the model on the specified text(s) on disk. Vocabulary Size The default vocabulary size for train_tokenizer() is 1,000 tokens. Although this is much lower than GPT-2's 50k vocab size, the smaller the vocab size, the easier it is to train the model (since it's more likely for the model to make a correct \"guess\"), and the model file size will be much smaller. from aitextgen.tokenizers import train_tokenizer train_tokenizer ( file_name ) This creates one file, aitextgen.tokenizer.json , which is needed to rebuild the tokenizer.","title":"Building a Custom Tokenizer."},{"location":"tutorials/model-from-scratch/#building-a-custom-dataset","text":"You can build a TokenDataset based off your custom Tokenizer, to be fed into the model. data = TokenDataset ( file_name , vocab_file = vocab_file , merges_file = merges_file , block_size = 32 )","title":"Building a Custom Dataset"},{"location":"tutorials/model-from-scratch/#building-a-custom-config","text":"Whenever you load a default 124M GPT-2 model, it uses a GPT2Config() under the hood. But you can create your own, with whatever parameters you want. The build_gpt2_config() function from aitextgen.utils gives you more control. config = build_gpt2_config ( vocab_size = 5000 , max_length = 32 , dropout = 0.0 , n_embd = 256 , n_layer = 8 , n_head = 8 ) A few notes on the inputs: vocab_size : Vocabulary size: this must match what you used to build the tokenizer! max_length : Context window for the GPT-2 model: this must match the block_size used in the TokenDataset! dropout : Dropout on various areas of the model to limit overfitting (you should likely keep at 0) n_embd : The embedding size for each vocab token. n_layers : Transformer layers n_head : Transformer heads Model Size GPT-2 Model size is directly proportional to vocab_size * embeddings .","title":"Building a Custom Config"},{"location":"tutorials/model-from-scratch/#training-the-custom-model","text":"You can instantiate an empty GPT-2 according to your custom config, and construct a custom tokenizer according to your vocab and merges file: ai = aitextgen ( tokenizer_file = tokenizer_file , config = config ) Training is done as normal. ai . train ( data , batch_size = 16 , num_steps = 5000 )","title":"Training the Custom Model"},{"location":"tutorials/model-from-scratch/#reloading-the-custom-model","text":"You'll always need to provide the tokenizer_file and the folder containing the pytorch_model.bin and config.json . ai = aitextgen ( model_folder = \"trained_model\" , tokenizer_file = \"aitextgen.tokenizer.json\" )","title":"Reloading the Custom Model"}]}